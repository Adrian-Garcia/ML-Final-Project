{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(\"ECOAS_MachineLearning_202013_puntajes.csv\", encoding=\"latin-1\")\n",
    "\n",
    "'''\n",
    "Classification of DocumentSentiment according to its value. \n",
    "(Changes from qualitative to quantitative variable)\n",
    "\n",
    "    Excellent   E\n",
    "    Good        G\n",
    "    Regular     R\n",
    "    Bad         B\n",
    "'''\n",
    "df['DocumentSentiment'] = np.where(df['DocumentSentiment'] > 0.50, 4, df['DocumentSentiment'])\n",
    "df['DocumentSentiment'] = np.where(np.logical_and(df['DocumentSentiment'] <= 0.50, df['DocumentSentiment'] > 0), 3, df['DocumentSentiment'])\n",
    "df['DocumentSentiment'] = np.where(np.logical_and(df['DocumentSentiment'] <= 0, df['DocumentSentiment'] > -0.50), 2, df['DocumentSentiment'])\n",
    "df['DocumentSentiment'] = np.where(df['DocumentSentiment'] <= -0.50, 1, df['DocumentSentiment'])\n",
    "\n",
    "df['DocumentSentiment'] = np.where(df['DocumentSentiment'] == 4.0, 'E', df['DocumentSentiment'])\n",
    "df['DocumentSentiment'] = np.where(df['DocumentSentiment'] == \"3.0\", 'G', df['DocumentSentiment'])\n",
    "df['DocumentSentiment'] = np.where(df['DocumentSentiment'] == \"2.0\", 'R', df['DocumentSentiment'])\n",
    "df['DocumentSentiment'] = np.where(df['DocumentSentiment'] == \"1.0\", 'B', df['DocumentSentiment'])\n",
    "\n",
    "df = df.drop(['ID', 'Rectoría', 'Campus', 'Tamaño', 'Latitud', 'Longitud', 'División', \n",
    "              'Departamento', 'AnonMateria', 'crn', 'Pregunta', 'NominaFict', 'TipoProfesor',\n",
    "              'ProfesorTitular', 'Comentarios', 'sentimiento', 'Categoria', 'Subcategoria', \n",
    "              'EntitiesList'], axis=1)\n",
    "\n",
    "# Get target and feature names\n",
    "target_names = df['DocumentSentiment'].unique().tolist()\n",
    "feature_names = ['APR', 'RET', 'REC']\n",
    "\n",
    "# Get number of features and target names\n",
    "n_targets = len(target_names)\n",
    "n_features = len(feature_names)\n",
    "\n",
    "# Separate features and target columns\n",
    "X = df[feature_names]\n",
    "y = df['DocumentSentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import seaborn as sn\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# import seaborn as sns\n",
    "\n",
    "# data = pd.read_csv(\"ECOAS_MachineLearning_202013_puntajes.csv\", encoding=\"latin-1\")\n",
    "\n",
    "# # Convert cualitative variables to quantitative \n",
    "# data['Categoria'] = pd.factorize(data.Categoria)[0]\n",
    "# data['Subcategoria'] = pd.factorize(data.Subcategoria)[0]\n",
    "# data['TipoProfesor'] = pd.factorize(data.TipoProfesor)[0]\n",
    "# data['Departamento'] = pd.factorize(data.Departamento)[0]\n",
    "# data['Rectoría'] = pd.factorize(data.Rectoría)[0]\n",
    "\n",
    "# # Convert Document Sentiment to cualitative\n",
    "# data['DocumentSentiment'] = np.where(data['DocumentSentiment'] >= 0.25, 4, data['DocumentSentiment'])\n",
    "# data['DocumentSentiment'] = np.where(data['DocumentSentiment'] < 0.25, 3, data['DocumentSentiment'])\n",
    "# data['DocumentSentiment'] = np.where(data['DocumentSentiment'] < 0, 2, data['DocumentSentiment'])\n",
    "# data['DocumentSentiment'] = np.where(data['DocumentSentiment'] < -0.25, 1, data['DocumentSentiment'])\n",
    "# # data['DocumentSentiment'] = np.where(data['DocumentSentiment'] == 4, 'Excellent', data['DocumentSentiment'])\n",
    "# # data['DocumentSentiment'] = np.where(data['DocumentSentiment'] == '3', 'Good', data['DocumentSentiment'])\n",
    "# # data['DocumentSentiment'] = np.where(data['DocumentSentiment'] == '2', 'Regular', data['DocumentSentiment'])\n",
    "# # data['DocumentSentiment'] = np.where(data['DocumentSentiment'] == '1', 'Bad', data['DocumentSentiment'])\n",
    "\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-1e1d580f654e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# df = pd.DataFrame(data,columns=['DocumentSentiment','Categoria','Subcategoria', 'TipoProfesor', 'Departamento', 'Rectoría', 'APR', 'RET', 'REC'])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'DocumentSentiment'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'APR'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'RET'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'REC'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# target_names = df['DocumentSentiment'].unique().tolist()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# feature_names = df.drop(['DocumentSentiment'], axis=1).columns.tolist()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# df = pd.DataFrame(data,columns=['DocumentSentiment','Categoria','Subcategoria', 'TipoProfesor', 'Departamento', 'Rectoría', 'APR', 'RET', 'REC'])\n",
    "df = pd.DataFrame(data,columns=['DocumentSentiment', 'APR', 'RET', 'REC'])\n",
    "\n",
    "# target_names = df['DocumentSentiment'].unique().tolist()\n",
    "# feature_names = df.drop(['DocumentSentiment'], axis=1).columns.tolist()\n",
    "\n",
    "# n_targets = len(target_names)\n",
    "# n_features = len(feature_names)\n",
    "\n",
    "# X = df[feature_names]\n",
    "# y = df['DocumentSentiment']\n",
    "\n",
    "#split dataset in features and target variable\n",
    "# feature_cols = ['Categoria','Subcategoria', 'TipoProfesor', 'Departamento', 'Rectoría', 'APR', 'RET', 'REC']\n",
    "feature_cols = ['APR', 'RET', 'REC']\n",
    "X = data[feature_cols] # Features (independent variables)\n",
    "y = data.DocumentSentiment # Target variable\n",
    "\n",
    "# newY = []\n",
    "# for i in y:\n",
    "#     newY.append( int(i * 100))\n",
    "    \n",
    "# y = newY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrMatrix = df.corr()\n",
    "sn.heatmap(corrMatrix, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn import utils\n",
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) \n",
    "# print('Shapes:', X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "# print('Target values distribution:\\nTrain:\\n', y_train.value_counts(normalize=True), '\\nTest:\\n', y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Decision Tree classifer object\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "clf = clf.fit(X_train,y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5062633960466778\n"
     ]
    }
   ],
   "source": [
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adrian Garcia Lopez\\anaconda3\\lib\\site-packages\\sklearn\\externals\\six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", FutureWarning)\n"
     ]
    },
    {
     "ename": "InvocationException",
     "evalue": "GraphViz's executables not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvocationException\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-53e4aa319b9f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m                 special_characters=True,feature_names = feature_names)\n\u001b[0;32m     10\u001b[0m \u001b[0mgraph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpydotplus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph_from_dot_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdot_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite_png\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'DocumentSentiment.png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_png\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pydotplus\\graphviz.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(path, f, prog)\u001b[0m\n\u001b[0;32m   1808\u001b[0m                 \u001b[1;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1809\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfrmt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1810\u001b[1;33m                 \u001b[0mprog\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprog\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprog\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprog\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1811\u001b[0m             )\n\u001b[0;32m   1812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pydotplus\\graphviz.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, path, prog, format)\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1917\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1918\u001b[1;33m                 \u001b[0mfobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1919\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1920\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pydotplus\\graphviz.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(self, prog, format)\u001b[0m\n\u001b[0;32m   1958\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1959\u001b[0m                 raise InvocationException(\n\u001b[1;32m-> 1960\u001b[1;33m                     'GraphViz\\'s executables not found')\n\u001b[0m\u001b[0;32m   1961\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1962\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mprog\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvocationException\u001b[0m: GraphViz's executables not found"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "import pydotplus\n",
    "\n",
    "dot_data = StringIO()\n",
    "export_graphviz(clf, out_file=dot_data, \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True,feature_names = feature_names)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "graph.write_png('DocumentSentiment.png')\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Decision Tree classifer object\n",
    "clf = DecisionTreeClassifier(criterion=\"entropy\", max_depth=3)\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "clf = clf.fit(X_train,y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.externals.six import StringIO  \n",
    "# from IPython.display import Image  \n",
    "# from sklearn.tree import export_graphviz\n",
    "# import pydotplus\n",
    "# dot_data = StringIO()\n",
    "# export_graphviz(clf, out_file=dot_data,  \n",
    "#                 filled=True, rounded=True,\n",
    "#                 special_characters=True, feature_names = feature_cols)\n",
    "# graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "# graph.write_png('DocumentSentiment2.png')\n",
    "# Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Decision Tree classifer object\n",
    "clf = DecisionTreeClassifier(criterion=\"gini\", max_depth=3)\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "clf = clf.fit(X_train,y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.externals.six import StringIO  \n",
    "# from IPython.display import Image  \n",
    "# from sklearn.tree import export_graphviz\n",
    "# import pydotplus\n",
    "# dot_data = StringIO()\n",
    "# export_graphviz(clf, out_file=dot_data,  \n",
    "#                 filled=True, rounded=True,\n",
    "#                 special_characters=True, feature_names = feature_cols)\n",
    "# graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "# graph.write_png('DocumentSentiment3.png')\n",
    "# Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import svm\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# parameters = [{'kernel': ['rbf'], 'gamma': [1,0.1,0.001,0.0001], 'C': [1, 10, 100, 1000]},\n",
    "#                 {'kernel': ['poly'], 'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "# svc = svm.SVC()\n",
    "# clf = GridSearchCV(estimator=svc, param_grid=parameters, cv=5, scoring='f1_macro')\n",
    "# clf = clf.fit(X_train, y_train)\n",
    "# print('Best parameters:\\n', clf.best_params_)\n",
    "# print('\\nModel parameters:\\n', clf.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "with sns.axes_style(\"white\"):\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print('Classification Report:\\n',metrics.classification_report(y_test, y_pred))\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15,8))\n",
    "    metrics.plot_confusion_matrix(clf, X_test, y_test, cmap=plt.cm.Blues, ax=axes[0])\n",
    "    metrics.plot_confusion_matrix(clf, X_test, y_test, cmap=plt.cm.Blues, normalize='true', ax=axes[1])\n",
    "    axes[0].set_title('Confussion Matrix')\n",
    "    axes[1].set_title('Confussion Matrix - Normalized')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale features and encode target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_y = np.array(y)\n",
    "y = new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X = np.array(X)\n",
    "X = new_X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder(categories='auto')\n",
    "Y = enc.fit_transform(y[:, np.newaxis]).toarray()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data set into training and testing\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_scaled, Y, test_size=0.5, random_state=2)\n",
    "\n",
    "n_features = X.shape[1]\n",
    "n_classes = Y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data sets\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "for target, target_name in enumerate(feature_cols):\n",
    "    X_plot = X[y == target]\n",
    "    plt.plot(X_plot[:, 0], X_plot[:, 1], linestyle='none', marker='o', label=target_name)\n",
    "# plt.xlabel(feature_names[0])\n",
    "# plt.ylabel(feature_names[1])\n",
    "plt.axis('equal')\n",
    "plt.legend();\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "for target, target_name in enumerate(feature_cols):\n",
    "    X_plot = X[y == target]\n",
    "    plt.plot(X_plot[:, 2], X_plot[:, 1], linestyle='none', marker='o', label=target_name)\n",
    "# plt.xlabel(feature_names[2])\n",
    "# plt.ylabel(feature_names[3])\n",
    "plt.axis('equal')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to ignore FutureWarning\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "def create_custom_model(input_dim, output_dim, nodes, n=1, name='model'):\n",
    "    def create_model():\n",
    "        # Create model\n",
    "        model = Sequential(name=name)\n",
    "        for i in range(n):\n",
    "            model.add(Dense(nodes, input_dim=input_dim, activation='relu'))\n",
    "        model.add(Dense(output_dim, activation='softmax'))\n",
    "\n",
    "        # Compile model\n",
    "        model.compile(loss='categorical_crossentropy', \n",
    "                      optimizer='adam', \n",
    "                      metrics=['accuracy'])\n",
    "        return model\n",
    "    return create_model\n",
    "\n",
    "models = [create_custom_model(n_features, n_classes, 8, i, 'model_{}'.format(i)) \n",
    "          for i in range(1, 4)]\n",
    "\n",
    "for create_model in models:\n",
    "    create_model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "history_dict = {}\n",
    "\n",
    "# TensorBoard Callback\n",
    "cb = TensorBoard()\n",
    "\n",
    "for create_model in models:\n",
    "    model = create_model()\n",
    "    print('Model name:', model.name)\n",
    "    history_callback = model.fit(X_train, Y_train,\n",
    "                                 batch_size=5,\n",
    "                                 epochs=50,\n",
    "                                 verbose=0,\n",
    "                                 validation_data=(X_test, Y_test),\n",
    "                                 callbacks=[cb])\n",
    "    score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "    \n",
    "    history_dict[model.name] = [history_callback, model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "create_model = create_custom_model(n_features, n_classes, 8, 3)\n",
    "\n",
    "estimator = KerasClassifier(build_fn=create_model, epochs=100, batch_size=5, verbose=0)\n",
    "scores = cross_val_score(estimator, X_scaled, Y, cv=10)\n",
    "print(\"Accuracy : {:0.2f} (+/- {:0.2f})\".format(scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
